{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ye7HsUeqYiM",
    "outputId": "a784394d-4a55-4213-b565-685d4d34f932"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6uAJIco6ndh",
    "outputId": "027bad8a-9a6c-401d-b7a9-998bb491166f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "from math import log\n",
    "import math\n",
    "import re\n",
    "from bs4 import BeautifulSoup  # For HTML cleaning\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAsfLjpP-2Tv",
    "outputId": "a667edb3-99ac-4391-8855-9afe00fc2b50"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Lowercase and remove non-alphanumeric characters.\"\"\"\n",
    "    text = str(text).lower()  # Ensure text is string\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Split text by whitespace and remove stopwords.\"\"\"\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "def build_vocabulary(documents, max_features=5000):\n",
    "    \"\"\"Build vocabulary of the top max_features tokens from the documents.\"\"\"\n",
    "    counter = Counter()\n",
    "    for doc in documents:\n",
    "        tokens = tokenize(doc)\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: i for i, (word, _) in enumerate(counter.most_common(max_features))}\n",
    "    return vocab\n",
    "\n",
    "def compute_idf(documents, vocab):\n",
    "    \"\"\"Compute inverse document frequency for each word in vocab.\"\"\"\n",
    "    N = len(documents)\n",
    "    df = np.zeros(len(vocab))\n",
    "    for doc in documents:\n",
    "        tokens = set(tokenize(doc))\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                df[vocab[token]] += 1\n",
    "    idf = np.log((N + 1) / (df + 1)) + 1\n",
    "    return idf\n",
    "\n",
    "def compute_tf_idf(documents, vocab, idf):\n",
    "    \"\"\"Compute TF-IDF for each document given a vocabulary and idf vector.\"\"\"\n",
    "    X = np.zeros((len(documents), len(vocab)))\n",
    "    for i, doc in enumerate(documents):\n",
    "        tokens = tokenize(doc)\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "        tf_counter = Counter(tokens)\n",
    "        total_count = len(tokens)\n",
    "        for token, count in tf_counter.items():\n",
    "            if token in vocab:\n",
    "                j = vocab[token]\n",
    "                tf = count / total_count\n",
    "                X[i, j] = tf * idf[j]\n",
    "    return X\n",
    "\n",
    "def multi_label_binarize(labels_list, label_mapping=None):\n",
    "    \"\"\"Convert list of comma-separated labels to a binary matrix.\"\"\"\n",
    "    if label_mapping is None:  # For training data\n",
    "        label_set = set()\n",
    "        split_labels = []\n",
    "        for labels in labels_list:\n",
    "            topics = [label.strip() for label in str(labels).split(',') if label.strip()]\n",
    "            split_labels.append(topics)\n",
    "            label_set.update(topics)\n",
    "        label_list = sorted(list(label_set))\n",
    "        label_mapping = {label: idx for idx, label in enumerate(label_list)}\n",
    "    else:  # For test data, use existing label_mapping\n",
    "        split_labels = []\n",
    "        for labels in labels_list:\n",
    "            topics = [label.strip() for label in str(labels).split(',') if label.strip()]\n",
    "            split_labels.append(topics)\n",
    "\n",
    "    Y = np.zeros((len(labels_list), len(label_mapping)))\n",
    "    for i, topics in enumerate(split_labels):\n",
    "        for topic in topics:\n",
    "            if topic in label_mapping:\n",
    "                Y[i, label_mapping[topic]] = 1\n",
    "    return Y, label_mapping\n",
    "\n",
    "def preprocess_data(train_csv_path, test_csv_path=None, max_features=5000, val_size=0.2):\n",
    "    \"\"\"Load CSV, clean documents, compute TF-IDF, binarize labels,\n",
    "       and split into training, validation, and test sets.\"\"\"\n",
    "\n",
    "    # Load and preprocess training data\n",
    "    df_train = pd.read_csv(train_csv_path)\n",
    "    df_train.dropna(subset=['document', 'category'], inplace=True)\n",
    "    df_train['document'] = df_train['document'].apply(clean_text)\n",
    "\n",
    "    train_docs = df_train['document'].tolist()\n",
    "    train_labels = df_train['category'].tolist()\n",
    "\n",
    "    # Build vocabulary and IDF from training data only\n",
    "    vocab = build_vocabulary(train_docs, max_features=max_features)\n",
    "    idf = compute_idf(train_docs, vocab)\n",
    "\n",
    "    # Compute TF-IDF for training data\n",
    "    X_full = compute_tf_idf(train_docs, vocab, idf)\n",
    "    Y_full, label_mapping = multi_label_binarize(train_labels)\n",
    "\n",
    "    # Split into train and validation\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X_full, Y_full, test_size=val_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # If test CSV is provided, preprocess it\n",
    "    if test_csv_path:\n",
    "        df_test = pd.read_csv(test_csv_path)\n",
    "        df_test.dropna(subset=['document', 'category'], inplace=True)\n",
    "        df_test['document'] = df_test['document'].apply(clean_text)\n",
    "\n",
    "        test_docs = df_test['document'].tolist()\n",
    "        test_labels = df_test['category'].tolist()\n",
    "\n",
    "        # Compute TF-IDF for test data using training vocab and IDF\n",
    "        X_test = compute_tf_idf(test_docs, vocab, idf)\n",
    "        Y_test, _ = multi_label_binarize(test_labels, label_mapping=label_mapping)  # Use training label mapping\n",
    "\n",
    "        return X_train, X_val, X_test, Y_train, Y_val, Y_test, vocab, label_mapping\n",
    "\n",
    "    # If no test CSV, return only train and val\n",
    "    return X_train, X_val, None, Y_train, Y_val, None, vocab, label_mapping\n",
    "\n",
    "# Usage example\n",
    "train_csv_path = \"/content/drive/MyDrive/news-article/train.csv\"  # Replace with your training CSV path\n",
    "test_csv_path = \"/content/drive/MyDrive/news-article/test.csv\"\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, vocab, label_mapping = preprocess_data(\n",
    "    train_csv_path, test_csv_path, max_features=5000, val_size=0.2\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors for use with your MLP\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_train shape: {y_train.shape}\")\n",
    "print(f\"Y_val shape: {y_val.shape}\")\n",
    "print(f\"Y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehnWjlbQ_FZh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, learning_rate,\n",
    "                 activation_function, loss_function, threshold=0.3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation_function\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.loss_function = loss_function.lower()\n",
    "        self.threshold = threshold\n",
    "        self.device = torch.device(device)  # Set device (GPU or CPU)\n",
    "\n",
    "    def _Initialize(self, input_size, output_size):\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in self.hidden_sizes:\n",
    "            weight = torch.randn(hidden_size, prev_size, dtype=torch.float32) * 0.01\n",
    "            bias = torch.randn(hidden_size, 1, dtype=torch.float32) * 0.01\n",
    "            self.weights.append(weight.to(self.device))\n",
    "            self.bias.append(bias.to(self.device))\n",
    "            prev_size = hidden_size\n",
    "        weight = torch.randn(output_size, prev_size, dtype=torch.float32) * 0.01\n",
    "        bias = torch.randn(output_size, 1, dtype=torch.float32) * 0.01\n",
    "        self.weights.append(weight.to(self.device))\n",
    "        self.bias.append(bias.to(self.device))\n",
    "\n",
    "    def _Activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return torch.relu(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return torch.tanh(x)\n",
    "        elif self.activation == 'linear':\n",
    "            return x\n",
    "\n",
    "    def _ActivationPrime(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return torch.where(x > 0, torch.tensor(1.0, dtype=x.dtype, device=self.device),\n",
    "                             torch.tensor(0.0, dtype=x.dtype, device=self.device))\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sig = torch.sigmoid(x)\n",
    "            return sig * (1 - sig)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - torch.tanh(x) ** 2\n",
    "        elif self.activation == 'linear':\n",
    "            return torch.ones_like(x)\n",
    "\n",
    "    def _Softmax(self, x):\n",
    "        exp_x = torch.exp(x - torch.max(x, dim=1, keepdim=True).values)\n",
    "        return exp_x / torch.sum(exp_x, dim=1, keepdim=True)\n",
    "\n",
    "    def _CrossEntropy(self, y, y_hat):\n",
    "        return -torch.mean(torch.sum(y * torch.log(y_hat + 1e-9), dim=1))\n",
    "\n",
    "    def _MSE(self, y, y_hat):\n",
    "        return torch.mean((y-y_hat)**2)\n",
    "\n",
    "    def _BCE(self, y, y_hat):\n",
    "        return -torch.mean(y * torch.log(y_hat + 1e-9) + (1 - y) * torch.log(1 - y_hat + 1e-9))\n",
    "\n",
    "    def _loss(self, y, y_hat):\n",
    "        if self.loss_function == \"crossentropy\":\n",
    "            return self._CrossEntropy(y, y_hat)\n",
    "        elif self.loss_function == \"bce\":\n",
    "            return self._BCE(y, y_hat)\n",
    "        return self._MSE(y, y_hat)\n",
    "\n",
    "    def _CrossEntropyderivative(self, y, y_hat):\n",
    "        return (y_hat-y)\n",
    "\n",
    "    def _MSEderivative(self, y, y_hat):\n",
    "        return 2 * (y_hat - y)\n",
    "\n",
    "    def _BCEderivative(self, y, y_hat):\n",
    "        # Derivative of Binary Cross Entropy: -(y/y_hat - (1-y)/(1-y_hat))\n",
    "        # return -(y / (y_hat + 1e-9) - (1 - y) / (1 - y_hat + 1e-9))\n",
    "        return (y_hat - y)\n",
    "\n",
    "    def _Forward(self, X):\n",
    "        activations = X\n",
    "        self.layer_inputs = []\n",
    "        self.z = []\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = torch.mm(activations, self.weights[i].T) + self.bias[i].T\n",
    "            activations = self._Activation(z)\n",
    "            self.layer_inputs.append(activations)\n",
    "            self.z.append(z)\n",
    "        z = torch.mm(activations, self.weights[-1].T) + self.bias[-1].T\n",
    "        self.layer_inputs.append(z)\n",
    "        return torch.sigmoid(z) if self.loss_function == \"bce\" else z\n",
    "\n",
    "    def _Backward(self, X, y, y_hat):\n",
    "        batch_size = X.shape[0]\n",
    "        if self.loss_function == \"crossentropy\":\n",
    "            dz = self._CrossEntropyderivative(y, y_hat) / batch_size\n",
    "        elif self.loss_function == \"bce\":\n",
    "            dz = self._BCEderivative(y, y_hat) / batch_size\n",
    "        else:\n",
    "            dz = self._MSEderivative(y, y_hat) / batch_size\n",
    "\n",
    "        grads_w = []\n",
    "        grads_b = []\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            dw = torch.mm(dz.T, self.layer_inputs[i - 1] if i > 0 else X)\n",
    "            db = torch.sum(dz, dim=0, keepdim=True).T\n",
    "            if i > 0:\n",
    "                dz = torch.mm(dz, self.weights[i]) * self._ActivationPrime(self.z[i - 1])\n",
    "            grads_w.insert(0, dw)\n",
    "            grads_b.insert(0, db)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads_w[i]\n",
    "            self.bias[i] -= self.learning_rate *grads_b[i]\n",
    "\n",
    "    def batch_fit(self, X, y, X_val, y_val, epochs, showloss=False):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        X_val, y_val = X_val.to(self.device), y_val.to(self.device)\n",
    "        self._Initialize(X.shape[1], y.shape[1])\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self._Forward(X)\n",
    "            train_loss = self._loss(y, y_hat)\n",
    "            train_losses.append(train_loss.item())\n",
    "            self._Backward(X, y, y_hat)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_val_hat = self._Forward(X_val)\n",
    "                val_loss = self._loss(y_val, y_val_hat)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            if epoch % 10 == 0 and showloss:\n",
    "                y_pred_binary = (y_val_hat >= self.threshold).float()\n",
    "                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} exact_match:{torch.mean(torch.all(y_pred_binary == y_val, dim=1).float())}')\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def Mini_batch_fit(self, X, y, X_val, y_val, epochs, batch_size, showloss=False):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        X_val, y_val = X_val.to(self.device), y_val.to(self.device)\n",
    "        dataset = TensorDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        self._Initialize(X.shape[1], y.shape[1])\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                y_hat = self._Forward(X_batch)\n",
    "                self._Backward(X_batch, y_batch, y_hat)\n",
    "            with torch.no_grad():\n",
    "                y_train_pred = self._Forward(X)\n",
    "                train_loss = self._loss(y, y_train_pred)\n",
    "                train_losses.append(train_loss.item())\n",
    "                y_val_pred = self._Forward(X_val)\n",
    "                val_loss = self._loss(y_val, y_val_pred)\n",
    "                val_losses.append(val_loss.item())\n",
    "            if epoch % 10 == 0 and showloss:\n",
    "                y_pred_binary = (y_val_pred >= self.threshold).float()\n",
    "                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} exact_match:{torch.mean(torch.all(y_pred_binary == y_val, dim=1).float())}')\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def SGD_fit(self, X, y, X_val, y_val, epochs, showloss=False):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        X_val, y_val = X_val.to(self.device), y_val.to(self.device)\n",
    "        self._Initialize(X.shape[1], y.shape[1])\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                X_batch = X[i].unsqueeze(0)\n",
    "                y_batch = y[i].unsqueeze(0)\n",
    "                y_hat = self._Forward(X_batch)\n",
    "                self._Backward(X_batch, y_batch, y_hat)\n",
    "            with torch.no_grad():\n",
    "                y_train_pred = self._Forward(X)\n",
    "                train_loss = self._loss(y, y_train_pred)\n",
    "                train_losses.append(train_loss.item())\n",
    "                y_val_pred = self._Forward(X_val)\n",
    "                val_loss = self._loss(y_val, y_val_pred)\n",
    "                val_losses.append(val_loss.item())\n",
    "            if  showloss:\n",
    "                y_pred_binary = (y_val_pred >= self.threshold).float()\n",
    "                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} exact_match:{torch.mean(torch.all(y_pred_binary == y_val, dim=1).float())}')\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.to(self.device)\n",
    "        y_hat = self._Forward(X)\n",
    "        if self.loss_function == \"crossentropy\":\n",
    "            return torch.argmax(y_hat, dim=1)\n",
    "        elif self.loss_function == \"bce\":\n",
    "            return (y_hat > self.threshold).float()\n",
    "        return y_hat\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = X.to(self.device)\n",
    "        return self._Forward(X)\n",
    "\n",
    "    def _RMSE(self, y_true, y_pred):\n",
    "        return torch.sqrt(torch.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "    def _R2_score(self, y_true, y_pred):\n",
    "        ss_total = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "        ss_residual = torch.sum((y_true - y_pred) ** 2)\n",
    "        return 1 - (ss_residual / ss_total)\n",
    "\n",
    "    def _hamming_error(self, y_true, y_pred):\n",
    "        if y_pred.dim() > 1 and y_pred.shape[1] > 1:\n",
    "            if self.loss_function == \"bce\":\n",
    "                y_pred_binary = (y_pred > self.threshold).float()\n",
    "            else:\n",
    "                y_pred_binary = y_pred\n",
    "        else:\n",
    "            y_pred_binary = y_pred\n",
    "        incorrect_predictions = torch.sum(torch.abs(y_true - y_pred_binary))\n",
    "        total_predictions = y_true.numel()\n",
    "        return incorrect_predictions / total_predictions\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        y_pred = self._Forward(X)\n",
    "        if self.loss_function == \"crossentropy\":\n",
    "            accuracy = torch.mean((torch.argmax(y_pred, dim=1) == torch.argmax(y, dim=1)).float())\n",
    "            loss = self._CrossEntropy(y, y_pred)\n",
    "            return {\n",
    "                'accuracy': accuracy.item(),\n",
    "                'loss': loss.item()\n",
    "            }\n",
    "        elif self.loss_function == \"bce\":\n",
    "            y_pred_binary = (y_pred > self.threshold).float()\n",
    "            loss = self._BCE(y, y_pred)\n",
    "            hamming_error = self._hamming_error(y, y_pred)\n",
    "            exact_match = torch.mean(torch.all(y_pred_binary == y, dim=1).float())\n",
    "            return {\n",
    "                'hamming_error': hamming_error.item(),\n",
    "                'exact_match': exact_match.item(),\n",
    "                'loss': loss.item()\n",
    "            }\n",
    "        else:\n",
    "            rmse = self._RMSE(y, y_pred)\n",
    "            r2 = self._R2_score(y, y_pred)\n",
    "            return {\n",
    "                'rmse': rmse.item(),\n",
    "                'r2': r2.item(),\n",
    "                'loss': self._MSE(y, y_pred).item()\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sByTeNkPJjxm"
   },
   "outputs": [],
   "source": [
    "\n",
    "def experiment(X_train, y_train, X_val, y_val, X_test, y_test, learning_rates, epochs_list,\n",
    "               architectures, activations, optimizers,\n",
    "               device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    # Move all data to GPU at the start\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "    results = []\n",
    "    act_opt_scores = {}  # Dictionary to store scores by (activation, optimizer) pair\n",
    "\n",
    "    # Iterate over all hyperparameter combinations\n",
    "    for lr, epoch, arch, act, opt in product(learning_rates, epochs_list, architectures, activations, optimizers):\n",
    "        print(f\"\\nTraining with lr={lr}, epochs={epoch}, arch={arch}, act={act}, opt={opt}\")\n",
    "        mlp = MLP(X_train.shape[1], y_train.shape[1], arch, lr, act, \"bce\", device=device)\n",
    "\n",
    "        # Train the model based on optimizer\n",
    "        if opt == \"Batch\":\n",
    "            train_losses, val_losses = mlp.batch_fit(X_train, y_train, X_val, y_val, epoch)\n",
    "        elif opt == \"Mini_Batch\":\n",
    "            train_losses, val_losses = mlp.Mini_batch_fit(X_train, y_train, X_val, y_val, epoch, batch_size=32)\n",
    "        else:  # SGD\n",
    "            train_losses, val_losses = mlp.SGD_fit(X_train, y_train, X_val, y_val, epoch)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_metrics = mlp.evaluate(X_val, y_val)\n",
    "        results.append((lr, epoch, arch, act, opt, val_metrics, train_losses, val_losses))\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_metrics = mlp.evaluate(X_test, y_test)\n",
    "        print(f\"Validation Metrics: {val_metrics}\")\n",
    "        print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "        # Store score for (activation, optimizer) pair (using hamming_error, lower is better)\n",
    "        key = (act, opt)\n",
    "        score = val_metrics['hamming_error']  # Negative because lower hamming_error is better\n",
    "        if key not in act_opt_scores or score < act_opt_scores[key][0]:\n",
    "            act_opt_scores[key] = (score, val_metrics, test_metrics)\n",
    "\n",
    "        # Plot training curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Config: lr={lr}, arch={arch}, act={act}, opt={opt}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Sort results by hamming_error (lower is better)\n",
    "    results.sort(key=lambda x: x[5]['hamming_error'])\n",
    "\n",
    "    # Print best configurations\n",
    "    print(\"\\nTop 3 Configurations (based on validation set):\")\n",
    "    for res in results[:3]:\n",
    "        print(f\"LR: {res[0]}, Epochs: {res[1]}, Arch: {res[2]}, Act: {res[3]}, Opt: {res[4]}, \"\n",
    "              f\"Val Metrics: {res[5]}\")\n",
    "\n",
    "    # Report ordered scores for each (activation, optimizer) combination\n",
    "    print(\"\\nOrdered Scores by Activation and Optimizer (based on validation set):\")\n",
    "    sorted_act_opt = sorted(act_opt_scores.items(), key=lambda x: x[1][0])  # Ascending because score is -hamming_error\n",
    "    for (act, opt), (score, val_metrics, test_metrics) in sorted_act_opt:\n",
    "        print(f\"Act: {act}, Opt: {opt}, Score (hamming_error): {val_metrics['hamming_error']:.4f}, \"\n",
    "              f\"Val Metrics: {val_metrics}, Test Metrics: {test_metrics}\")\n",
    "\n",
    "    # Identify and train best configuration\n",
    "    best_config = results[0]\n",
    "    print(f\"\\nBest Configuration: LR={best_config[0]}, Epochs={best_config[1]}, Arch={best_config[2]}, \"\n",
    "          f\"Act={best_config[3]}, Opt={best_config[4]}\")\n",
    "\n",
    "    best_mlp = MLP(X_train.shape[1], y_train.shape[1], best_config[2], best_config[0],\n",
    "                   best_config[3], \"bce\", device=device)\n",
    "    if best_config[4] == \"Batch\":\n",
    "        train_losses, val_losses = best_mlp.batch_fit(X_train, y_train, X_val, y_val, best_config[1])\n",
    "    elif best_config[4] == \"Mini_Batch\":\n",
    "        train_losses, val_losses = best_mlp.Mini_batch_fit(X_train, y_train, X_val, y_val, best_config[1], batch_size=32)\n",
    "    else:\n",
    "        train_losses, val_losses = best_mlp.SGD_fit(X_train, y_train, X_val, y_val, best_config[1])\n",
    "\n",
    "    # Final evaluation on test set for best config\n",
    "    best_test_metrics = best_mlp.evaluate(X_test, y_test)\n",
    "    print(f\"Final Test Metrics for Best Config: {best_test_metrics}\")\n",
    "\n",
    "    # Plot training curves for best config\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Best Config: {best_config[3]}-{best_config[4]}-LR{best_config[0]}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results, best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBDxzq6L_jWY",
    "outputId": "9ebfeff3-b02b-4327-8d90-7188249e4407"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "input_size = X_train.shape[1]\n",
    "mlp = MLP(input_size=input_size, output_size=y_train.shape[1], hidden_sizes= [256,128,64], learning_rate=0.1, activation_function='relu', loss_function='bce',threshold = 0.5)\n",
    "\n",
    "#training using mini_batch\n",
    "epochs = 200\n",
    "mlp.Mini_batch_fit(X_train,y_train, X_val, y_val,epochs,32,showloss = True)\n",
    "metrics = mlp.evaluate(X_val, y_val)\n",
    "\n",
    "print(f\"mini Batch Grad descent\")\n",
    "print(f'exact_match:{metrics[\"exact_match\"]}')\n",
    "print(f'Hamming error:{metrics[\"hamming_error\"]}')\n",
    "\n",
    "end = time.time()\n",
    "print(f\"time took:{(end-start):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "62IipqgGAXcM",
    "outputId": "d7852495-d7c3-44c0-e9e4-a04580d09a91"
   },
   "outputs": [],
   "source": [
    "# Example usage (you need to define your data first)\n",
    "# Hyperparameters\n",
    "learning_rates = [0.05,0.1]\n",
    "epochs_list = [100,200]\n",
    "architectures = [[1024,64], [256,128,64]]\n",
    "activations = ['relu', 'sigmoid','tanh']\n",
    "optimizers = ['Batch', 'Mini_Batch']\n",
    "\n",
    "# For multi-label classification\n",
    "results, best_config = experiment(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                                learning_rates, epochs_list, architectures, activations,\n",
    "                                optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpeYuOBOG962",
    "outputId": "54d6c925-31d4-46df-ef24-6c3c60c75948"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "input_size = X_train.shape[1]\n",
    "mlp = MLP(input_size=input_size, output_size=y_train.shape[1], hidden_sizes= [256,128,64], learning_rate=0.01, activation_function='relu', loss_function='bce',threshold = 0.5)\n",
    "\n",
    "#training using mini_batch\n",
    "epochs = 10\n",
    "mlp.SGD_fit(X_train,y_train, X_val, y_val,epochs,showloss = True)\n",
    "metrics = mlp.evaluate(X_val, y_val)\n",
    "\n",
    "print(f\"mini Batch Grad descent\")\n",
    "print(f'exact_match:{metrics[\"exact_match\"]}')\n",
    "print(f'Hamming error:{metrics[\"hamming_error\"]}')\n",
    "\n",
    "end = time.time()\n",
    "print(f\"time took:{(end-start):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WkvgvvYMCFGt",
    "outputId": "d266f097-f180-49d9-f9af-f22a53845d1f"
   },
   "outputs": [],
   "source": [
    "# Example usage (you need to define your data first)\n",
    "# Hyperparameters\n",
    "learning_rates = [0.005, 0.01]\n",
    "epochs_list = [10]\n",
    "architectures = [[1024,64], [256,128,64]]\n",
    "activations = ['relu', 'sigmoid','tanh']\n",
    "optimizers = ['sgd']\n",
    "\n",
    "# For multi-label classification\n",
    "results, best_config = experiment(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                                learning_rates, epochs_list, architectures, activations,\n",
    "                                optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eTSUAtCd9Q8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
