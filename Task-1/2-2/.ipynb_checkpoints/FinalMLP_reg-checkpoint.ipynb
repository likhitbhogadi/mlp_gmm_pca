{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031743a5-de03-41fb-be9d-c4496cca27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d2f19-99af-4b70-bd65-f603e9ffc2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Bengaluru_House_Data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf50fd-79a1-4493-8c63-d29d02a3e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')  # For both numerical and categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a0f8e-c4d1-4c5e-8f50-f4c4c8d9898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7e709-7cf2-4199-8a4d-e2fad5e79490",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['society'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525511c-fc50-4f17-9294-68c672634916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `data` is your original DataFrame\n",
    "df_cleaned = data.copy()  # Make a copy to avoid modifying the original data\n",
    "\n",
    "for col in df_cleaned.columns:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        if df_cleaned[col].dtype in [np.float64, np.int64]:\n",
    "            median_val = df_cleaned[col].median()\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(median_val)  # Direct assignment\n",
    "            print(f\"Filled missing numeric values in '{col}' with median: {median_val}\")\n",
    "        else:\n",
    "            mode_val = df_cleaned[col].mode()[0]\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(mode_val)  # Direct assignment\n",
    "            print(f\"Filled missing categorical values in '{col}' with mode: {mode_val}\")\n",
    "\n",
    "# Check if missing values are filled\n",
    "print(df_cleaned.isnull().sum())  # Should print 0 for all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8e78e-75ec-422e-9087-0d41351d79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acec3e-1bdc-4b3e-a665-00eee66055f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your dataframe (assuming it's already loaded as 'df')\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_columns = ['area_type', 'availability', 'location', 'size']\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot each categorical column\n",
    "for i, col in enumerate(categorical_columns, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.countplot(y=df_cleaned[col], order=df_cleaned[col].value_counts().index, hue=df_cleaned[col], legend=False, palette=\"viridis\")\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(col)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af418bdf-5e72-4256-a0dc-6daeaebe1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns of interest\n",
    "columns = [\"bath\", \"balcony\", \"price\"]\n",
    "\n",
    "# Plot histograms for each column\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, col in enumerate(columns, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    df_cleaned[col].hist(bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769169b-1e52-435e-ab24-dd4ca35385aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d799d84-b6ea-4f3d-b2da-96a3d6c15797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns = [\"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1277a-e176-4b17-9138-3d0090beeb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#one hot encoding area_type\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Set sparse_output=False to get a dense array\n",
    "encoded_array = encoder.fit_transform(data[['area_type']])\n",
    "# Convert back to DataFrame with proper column names\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['area_type']))\n",
    "# Concatenate with original DataFrame (if needed)\n",
    "df_final = pd.concat([df_final, encoded_df], axis=1).drop(columns=['area_type'])\n",
    "\n",
    "encoded_array = encoder.fit_transform(data[['availability']])\n",
    "# Convert back to DataFrame with proper column names\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['availability']))\n",
    "# Concatenate with original DataFrame (if needed)\n",
    "df_final = pd.concat([df_final, encoded_df], axis=1).drop(columns=['availability'])\n",
    "\n",
    "# encoded_array = encoder.fit_transform(data[['location']])\n",
    "# # Convert back to DataFrame with proper column names\n",
    "# encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['location']))\n",
    "# # Concatenate with original DataFrame (if needed)\n",
    "# df_final = pd.concat([df_final, encoded_df], axis=1).drop(columns=['location'])\n",
    "\n",
    "encoded_array = encoder.fit_transform(data[['size']])\n",
    "# Convert back to DataFrame with proper column names\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(['size']))\n",
    "# Concatenate with original DataFrame (if needed)\n",
    "df_final = pd.concat([df_final, encoded_df], axis=1).drop(columns=['size'])\n",
    "\n",
    "df_final.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb7f20-a216-44e4-b978-e0f43a2fc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion function to encode the total_sqft\n",
    "# Conversion factors to square feet\n",
    "conversion_factors = {\n",
    "    \"Sq. Meter\": 10.764,  # 1 square meter = 10.764 square feet\n",
    "    \"Sq. Yards\": 9,       # 1 square yard = 9 square feet\n",
    "    \"Acres\": 43560,       # 1 acre = 43560 square feet\n",
    "    \"Cents\": 435.6,       # 1 cent = 435.6 square feet\n",
    "    \"Guntha\": 1089,       # 1 guntha = 1089 square feet\n",
    "    \"Grounds\": 2400,      # 1 ground = 2400 square feet\n",
    "    \"Perch\": 272.25       # 1 perch = 272.25 square feet\n",
    "}\n",
    "\n",
    "def convert_to_sqft(value):\n",
    "    try:\n",
    "        value = str(value).strip()  # Ensure value is a string\n",
    "        \n",
    "        # Case 1: Direct number (integer or float) (e.g., '1056', '1034.45')\n",
    "        if re.match(r'^\\d+(\\.\\d+)?$', value):  \n",
    "            return float(value)  # Convert to float\n",
    "        \n",
    "        # Case 2: Range (e.g., '1133 - 1384') â†’ Take the average\n",
    "        if '-' in value:\n",
    "            low, high = value.split('-')\n",
    "            return (float(low.strip()) + float(high.strip())) / 2\n",
    "\n",
    "        # Case 3: Handling area units (e.g., '4125Perch', '1000Sq. Meter')\n",
    "        match = re.match(r'([\\d.]+)\\s*([A-Za-z. ]+)', value)\n",
    "        if match:\n",
    "            num, unit = match.groups()\n",
    "            num = float(num)\n",
    "            unit = unit.strip()\n",
    "            \n",
    "            # Match known area units\n",
    "            for key in conversion_factors.keys():\n",
    "                if key in unit:\n",
    "                    return num * conversion_factors[key]\n",
    "\n",
    "        # Case 4: If nothing matched, return None\n",
    "        print(value)\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting value: {value}, {e}\")\n",
    "        return None\n",
    "# Apply conversion\n",
    "df_final['total_sqft'] = df_final['total_sqft'].apply(convert_to_sqft).astype(float)\n",
    "\n",
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557bcb9-9fe6-4746-9605-248c05a57b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['total_sqft', 'bath', 'balcony', 'price']\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)  # First quartile (25th percentile)\n",
    "    Q3 = df[column].quantile(0.75)  # Third quartile (75th percentile)\n",
    "    IQR = Q3 - Q1  # Interquartile range\n",
    "    lower_bound = Q1 - 1.5 * IQR  # Lower bound\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Upper bound\n",
    "\n",
    "    # Keep only values within bounds\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Apply the function to all numeric columns\n",
    "for col in numeric_cols:\n",
    "    df_final = remove_outliers_iqr(df_final, col)\n",
    "\n",
    "# Print new shape after outlier removal\n",
    "print(f\"New dataset shape: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddac635-890f-44e7-87a0-ac71038b9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "# df_final['availability'] = scaler.fit_transform(df_final[['availability']])\n",
    "# df_final['size'] = scaler.fit_transform(df_final[['size']])\n",
    "df_final['bath'] = scaler.fit_transform(df_final[['bath']])\n",
    "df_final['balcony'] = scaler.fit_transform(df_final[['balcony']])\n",
    "df_final['total_sqft'] = scaler.fit_transform(df_final[['total_sqft']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e55cd6-5108-4681-acf5-dc11e5465586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Drop the 'price' column to get X (independent variables)     [['total_sqft', 'bath', 'balcony']]\n",
    "X = df_final.drop(columns=['price'])\n",
    "\n",
    "# Select only the 'price' column for y (target variable)\n",
    "y = df_final['price']\n",
    "\n",
    "# Splitting data into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split temp data into validation (15%) and test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# print(X_train.dtypes)  # Ensure all are numeric\n",
    "# print(X_train.isna().sum())  # Ensure no missing values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)  # Ensure it's 2D\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(\"Training Tensor Shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation Tensor Shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test Tensor Shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c101a-ce91-4765-b1ff-e39c8ca9c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, learning_rate, activation_function, loss_function):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation_function\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.loss_function = loss_function.lower()\n",
    "\n",
    "    def _Initialize(self, input_size, output_size):\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in self.hidden_sizes:\n",
    "            self.weights.append(torch.randn(hidden_size, prev_size, dtype=torch.float32) * 0.01)\n",
    "            self.bias.append(torch.randn(hidden_size, 1, dtype=torch.float32) * 0.01)\n",
    "            prev_size = hidden_size\n",
    "    \n",
    "        self.weights.append(torch.randn(output_size, prev_size, dtype=torch.float32) * 0.01)\n",
    "        self.bias.append(torch.randn(output_size, 1, dtype=torch.float32) * 0.01)\n",
    "\n",
    "    def _Activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return torch.relu(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return torch.tanh(x)\n",
    "        elif self.activation == 'linear':\n",
    "            return x\n",
    "\n",
    "    def _MSE(self, y, y_hat):\n",
    "        return torch.mean((y - y_hat) ** 2)\n",
    "\n",
    "    def _loss(self, y, y_hat):\n",
    "        return self._MSE(y, y_hat)\n",
    "\n",
    "    def _Forward(self, X):\n",
    "        activations = X\n",
    "        self.layer_inputs = []\n",
    "        self.z = []\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = torch.mm(activations, self.weights[i].T) + self.bias[i].T\n",
    "            activations = self._Activation(z)\n",
    "            self.layer_inputs.append(activations)\n",
    "            self.z.append(z)\n",
    "\n",
    "        z = torch.mm(activations, self.weights[-1].T) + self.bias[-1].T\n",
    "        self.layer_inputs.append(z)\n",
    "        return z\n",
    "        \n",
    "    def _Backward(self, X, y, y_hat):\n",
    "        dz = 2 * (y_hat - y) / X.shape[0]\n",
    "        grads_w = []\n",
    "        grads_b = []\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            dw = torch.mm(dz.T, self.layer_inputs[i - 1] if i > 0 else X)\n",
    "            db = torch.sum(dz, dim=0, keepdim=True).T\n",
    "            if i > 0:\n",
    "                dz = torch.mm(dz, self.weights[i]) * (self.z[i - 1] > 0).float()\n",
    "            grads_w.insert(0, dw)\n",
    "            grads_b.insert(0, db)\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads_w[i]\n",
    "            self.bias[i] -= self.learning_rate * grads_b[i]\n",
    "   \n",
    "    def batch_fit(self, X, y, X_val, y_val, epochs, showloss=False):\n",
    "        self._Initialize(X.shape[1], y.shape[1])\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self._Forward(X)\n",
    "            train_loss = self._loss(y, y_hat)\n",
    "            train_losses.append(train_loss.item())\n",
    "            self._Backward(X, y, y_hat)\n",
    "            \n",
    "            # Compute validation loss\n",
    "            with torch.no_grad():\n",
    "                y_val_pred = self._Forward(X_val)\n",
    "                val_loss = self._loss(y_val, y_val_pred)\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            if epoch % 10 == 0 and showloss:\n",
    "                print(f'Epoch {epoch}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        return train_losses, val_losses\n",
    "            \n",
    "    def Mini_batch_fit(self, X, y, X_val, y_val, epochs, batch_size, showloss=False):\n",
    "        dataset = TensorDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        self._Initialize(X.shape[1], y.shape[1])\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                y_hat = self._Forward(X_batch)\n",
    "                self._Backward(X_batch, y_batch, y_hat)\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.predict(X)\n",
    "                train_loss = self._loss(y, y_pred)\n",
    "                train_losses.append(train_loss.item())\n",
    "                y_val_pred = self.predict(X_val)\n",
    "                val_loss = self._loss(y_val, y_val_pred)\n",
    "                val_losses.append(val_loss.item())\n",
    "            if epoch % 10 == 0 and showloss:\n",
    "                print(f'Epoch {epoch}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        return train_losses, val_losses\n",
    "            \n",
    "    def SGD_fit(self, X, y, X_val, y_val, epochs, showloss=False):\n",
    "        self._Initialize(X.shape[1], y.shape[1])\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                X_batch = X[i].unsqueeze(0)\n",
    "                y_batch = y[i].unsqueeze(0)\n",
    "                y_hat = self._Forward(X_batch)\n",
    "                self._Backward(X_batch, y_batch, y_hat)\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.predict(X)\n",
    "                train_loss = self._loss(y, y_pred)\n",
    "                train_losses.append(train_loss.item())\n",
    "                y_val_pred = self.predict(X_val)\n",
    "                val_loss = self._loss(y_val, y_val_pred)\n",
    "                val_losses.append(val_loss.item())\n",
    "            if showloss:\n",
    "                print(f'Epoch {epoch}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self._Forward(X)\n",
    "    \n",
    "    def _RMSE(self, y_true, y_pred):\n",
    "        return torch.sqrt(torch.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "    def _R2_score(self, y_true, y_pred):\n",
    "        ss_total = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "        ss_residual = torch.sum((y_true - y_pred) ** 2)\n",
    "        return 1 - (ss_residual / ss_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d1af5-6953-40c5-bd9b-e18df1ea4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(X_train, y_train, X_val, y_val, X_test, y_test, learning_rates, epochs_list,\n",
    "               architectures, activations, optimizers,\n",
    "               device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    # Move all data to GPU at the start\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "    results = []\n",
    "    act_opt_scores = {}  # Dictionary to store scores by (activation, optimizer) pair\n",
    "\n",
    "    # Iterate over all hyperparameter combinations\n",
    "    for lr, epoch, arch, act, opt in product(learning_rates, epochs_list, architectures, activations, optimizers):\n",
    "        print(f\"\\nTraining with lr={lr}, epochs={epoch}, arch={arch}, act={act}, opt={opt}\")\n",
    "        mlp = MLP(X_train.shape[1], y_train.shape[1], arch, lr, act, 'mse')\n",
    "\n",
    "        # Train the model based on optimizer\n",
    "        if opt == \"Batch\":\n",
    "            train_losses, val_losses = mlp.batch_fit(X_train, y_train, X_val, y_val, epoch)\n",
    "        elif opt == \"Mini_Batch\":\n",
    "            train_losses, val_losses = mlp.Mini_batch_fit(X_train, y_train, X_val, y_val, epoch, batch_size=32)\n",
    "        else:  # SGD\n",
    "            train_losses, val_losses = mlp.SGD_fit(X_train, y_train, X_val, y_val, epoch)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = mlp.predict(X_val)\n",
    "            val_mse = mlp._MSE(y_val, y_val_pred).item()\n",
    "            val_rmse = mlp._RMSE(y_val, y_val_pred).item()\n",
    "            val_r2 = mlp._R2_score(y_val, y_val_pred).item()\n",
    "        val_metrics = {'mse': val_mse, 'rmse': val_rmse, 'r2': val_r2}\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = mlp.predict(X_test)\n",
    "            test_mse = mlp._MSE(y_test, y_test_pred).item()\n",
    "            test_rmse = mlp._RMSE(y_test, y_test_pred).item()\n",
    "            test_r2 = mlp._R2_score(y_test, y_test_pred).item()\n",
    "        test_metrics = {'mse': test_mse, 'rmse': test_rmse, 'r2': test_r2}\n",
    "\n",
    "        results.append((lr, epoch, arch, act, opt, val_metrics, train_losses, val_losses))\n",
    "\n",
    "        print(f\"Validation Metrics: {val_metrics}\")\n",
    "        print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "        # Store score for (activation, optimizer) pair (using mse, lower is better)\n",
    "        key = (act, opt)\n",
    "        score = val_metrics['mse']  # Lower MSE is better\n",
    "        if key not in act_opt_scores or score < act_opt_scores[key][0]:\n",
    "            act_opt_scores[key] = (score, val_metrics, test_metrics)\n",
    "\n",
    "        # Plot training curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Config: lr={lr}, arch={arch}, act={act}, opt={opt}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Sort results by mse (lower is better)\n",
    "    results.sort(key=lambda x: x[5]['mse'])\n",
    "\n",
    "    # Print best configurations\n",
    "    print(\"\\nTop 3 Configurations (based on validation set):\")\n",
    "    for res in results[:3]:\n",
    "        print(f\"LR: {res[0]}, Epochs: {res[1]}, Arch: {res[2]}, Act: {res[3]}, Opt: {res[4]}, \"\n",
    "              f\"Val Metrics: {res[5]}\")\n",
    "\n",
    "    # Report ordered scores for each (activation, optimizer) combination\n",
    "    print(\"\\nOrdered Scores by Activation and Optimizer (based on validation set):\")\n",
    "    sorted_act_opt = sorted(act_opt_scores.items(), key=lambda x: x[1][0])  # Ascending because score is mse\n",
    "    for (act, opt), (score, val_metrics, test_metrics) in sorted_act_opt:\n",
    "        print(f\"Act: {act}, Opt: {opt}, Score (mse): {val_metrics['mse']:.4f}, \"\n",
    "              f\"Val Metrics: {val_metrics}, Test Metrics: {test_metrics}\")\n",
    "\n",
    "    # Identify and train best configuration\n",
    "    best_config = results[0]\n",
    "    print(f\"\\nBest Configuration: LR={best_config[0]}, Epochs={best_config[1]}, Arch={best_config[2]}, \"\n",
    "          f\"Act={best_config[3]}, Opt={best_config[4]}\")\n",
    "\n",
    "    best_mlp = MLP(X_train.shape[1], y_train.shape[1], best_config[2], best_config[0],\n",
    "                   best_config[3], \"mse\")  # Changed from \"bce\" to \"mse\" to match class\n",
    "    if best_config[4] == \"Batch\":\n",
    "        train_losses, val_losses = best_mlp.batch_fit(X_train, y_train, X_val, y_val, best_config[1])\n",
    "    elif best_config[4] == \"Mini_Batch\":\n",
    "        train_losses, val_losses = best_mlp.Mini_batch_fit(X_train, y_train, X_val, y_val, best_config[1], batch_size=32)\n",
    "    else:\n",
    "        train_losses, val_losses = best_mlp.SGD_fit(X_train, y_train, X_val, y_val, best_config[1])\n",
    "\n",
    "    # Final evaluation on test set for best config\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = best_mlp.predict(X_test)\n",
    "        test_mse = best_mlp._MSE(y_test, y_test_pred).item()\n",
    "        test_rmse = best_mlp._RMSE(y_test, y_test_pred).item()\n",
    "        test_r2 = best_mlp._R2_score(y_test, y_test_pred).item()\n",
    "    best_test_metrics = {'mse': test_mse, 'rmse': test_rmse, 'r2': test_r2}\n",
    "    print(f\"Final Test Metrics for Best Config: {best_test_metrics}\")\n",
    "\n",
    "    # Plot training curves for best config\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Best Config: {best_config[3]}-{best_config[4]}-LR{best_config[0]}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results, best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75b56b-f24c-4c90-8312-60d1ed85c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter ranges\n",
    "learning_rates = [0.001, 0.01]\n",
    "epochs_list = [100]\n",
    "architectures = [[64, 32], [128, 64, 32]]\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "optimizers = ['Batch', 'Mini_Batch']\n",
    "\n",
    "# Run experiment\n",
    "results, best_config = experiment(X_train, y_train, X_val, y_val, X_test, y_test, learning_rates, epochs_list, architectures, activations, optimizers)\n",
    "\n",
    "# Define hyperparameter ranges for SGD\n",
    "learning_rates = [0.001, 0.01]\n",
    "epochs_list = [10]\n",
    "architectures = [[64, 32], [128, 64, 32]]\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "optimizers = ['SGD']\n",
    "\n",
    "results, best_config = experiment(X_train, y_train, X_val, y_val, X_test, y_test, learning_rates, epochs_list, architectures, activations, optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250e2c8-ba15-427e-a0ad-6a7dd1fe8a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
