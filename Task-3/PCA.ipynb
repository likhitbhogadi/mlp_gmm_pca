{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bec83b-9fcb-4008-92a3-6b2dfe1788b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5084d990-f1a1-4511-97b4-3d128490dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "mnist_train = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04548d5e-1958-47d4-b4da-8656167f7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.stack([mnist_test[i][0] for i in range(len(mnist_test))]).numpy()\n",
    "y_test = np.array([label for _, label in mnist_test])\n",
    "\n",
    "# Check the shape\n",
    "print(\"Test Images Shape:\", X_test.shape)  # (10000, 784)\n",
    "print(\"Test Labels Shape:\", y_test.shape)  # (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf217144-1660-4c27-b97a-d3b5105ec8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to numpy arrays\n",
    "X = torch.stack([mnist_train[i][0] for i in range(len(mnist_train))]).numpy()\n",
    "y = np.array([mnist_train[i][1] for i in range(len(mnist_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c5803-ea1b-4389-a20a-6134fd0b8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Images Shape:\", X.shape)  # (60000, 784)\n",
    "print(\"Train Labels Shape:\", y.shape)  # (60000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f293343-35b0-402c-8a13-acc6292a8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample 1000 images uniformly across classes\n",
    "def sample_uniform(X, y, num_samples=1000):\n",
    "    class_counts = Counter(y)\n",
    "    samples_per_class = num_samples // len(class_counts)\n",
    "    sampled_X, sampled_y = [], []\n",
    "    for label in class_counts.keys():\n",
    "        indices = np.where(y == label)[0]\n",
    "        chosen_indices = np.random.choice(indices, samples_per_class, replace=False)\n",
    "        sampled_X.extend(X[chosen_indices])\n",
    "        sampled_y.extend(y[chosen_indices])\n",
    "    return np.array(sampled_X), np.array(sampled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b3ced-dfe9-4ea7-80f1-6c857cca935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sample 1000 images\n",
    "X_sampled, y_sampled = sample_uniform(X, y)\n",
    "\n",
    "# PCA Implementation\n",
    "def pca(X, num_components=None):\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_centered = X - X_mean\n",
    "    covariance_matrix = np.cov(X_centered, rowvar=False)\n",
    "    eigenvalues, eigenvectors = eig(covariance_matrix)\n",
    "    eigenvalues = np.real(eigenvalues)\n",
    "    eigenvectors = np.real(eigenvectors)\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    if num_components is not None:\n",
    "        eigenvectors = eigenvectors[:, :num_components]\n",
    "    X_projected = np.dot(X_centered, eigenvectors)\n",
    "    return X_projected, eigenvectors, X_mean, eigenvalues\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9663f-9b12-4796-8069-ea51f0b6ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA and explained variance\n",
    "X_projected, eigenvectors, X_mean, eigenvalues = pca(X_sampled)\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Explained Variance vs. Number of Principal Components\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a726d7-c044-4774-91d1-2af6d02a2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 2 principal components\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1], c=y_sampled, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label=\"Digit Label\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"2D Projection of MNIST using PCA\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ab9e7-d5af-496b-80bd-c004fe8cacc0",
   "metadata": {},
   "source": [
    "# **Observations on the PCA Projection of MNIST**\n",
    "\n",
    "### 1. **Separation of Digit Clusters**\n",
    "   - Some digits (e.g., **0 and 1**) form relatively distinct clusters, while others (e.g., **3, 5, and 8**) are more spread out and overlapping.\n",
    "   - This suggests that certain digits have more unique features, making them easier to separate in a lower-dimensional space.\n",
    "\n",
    "### 2. **Overlap of Multiple Digits**\n",
    "   - Many points from different digit classes overlap, indicating that a simple **2D projection does not fully separate the digits**.\n",
    "   - This suggests that some digits share similar pixel distributions, making them harder to distinguish using only two principal components.\n",
    "\n",
    "### 3. **Distribution of Data**\n",
    "   - The data appears to be **denser near the origin**, with most points clustered around **(0,0)**.\n",
    "   - This indicates that **the first two principal components capture a significant portion of the variance**, but additional components may be needed for better separation.\n",
    "\n",
    "### 4. **Non-linearity of the Data**\n",
    "   - The overlapping regions suggest that **digits in MNIST might not be fully linearly separable in 2D**.\n",
    "   - This aligns with the need for **non-linear techniques like t-SNE or UMAP** for better visualization.\n",
    "\n",
    "### 5. **Insights for Classification**\n",
    "   - A classifier using only **PC1 and PC2 as features would struggle** due to the overlapping digit regions.\n",
    "   - More principal components or other feature extraction techniques would be needed for accurate **digit classification**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07f73b-7e7d-4e25-abe0-e575fd1007b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original shape: {X_sampled.shape}\")\n",
    "\n",
    "# Display the first 10 original images only once\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(X_sampled[i].reshape(28, 28), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n",
    "\n",
    "for dim in [500, 300, 150, 30]:\n",
    "    X_reduced, eigenvectors, X_mean,_ = pca(X_sampled, dim)\n",
    "    print(f\"Projected dataset to {dim} dimensions. Shape: {X_reduced.shape}\")\n",
    "    \n",
    "    # Display first 10 reconstructed images\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "    for i in range(5):\n",
    "        reconstructed = np.dot(X_reduced[i], eigenvectors.T) + X_mean\n",
    "        axes[i].imshow(reconstructed.reshape(28, 28), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eeca40-116a-4ce4-bc11-d008641c6699",
   "metadata": {},
   "source": [
    "# **Observations on Dimensionality Reduction and Reconstruction of MNIST Digits**\n",
    "\n",
    "### **1. Impact of Dimensionality Reduction on Image Quality**\n",
    "   - As the number of principal components decreases, the reconstructed images progressively lose finer details and become more **blurred or distorted**.\n",
    "   - The original images (784 dimensions) retain **sharp edges and clear digit structures**, whereas the images reconstructed from **30 dimensions appear highly distorted**.\n",
    "\n",
    "### **2. Retaining Important Features**\n",
    "   - **500 dimensions:** The reconstructed images are **almost identical** to the originals, indicating that most critical features are preserved.\n",
    "   - **300 dimensions:** The images remain recognizable, but slight blurring and **loss of sharpness** at the place of gray and white region start to appear.\n",
    "   - **150 dimensions:** Some strokes appear **less defined**, and minor distortions can be seen, but the digits are still distinguishable.\n",
    "   - **30 dimensions:** The images are **heavily distorted**, with some digits losing their basic structure, making them **harder to recognize**.\n",
    "\n",
    "### **3. Conclusion**\n",
    "   - **500-300 dimensions** provide a **good balance** between compression and reconstruction quality.\n",
    "   - **150 dimensions** can still capture recognizable digit features but may struggle in fine-grained classification tasks.\n",
    "   - **30 dimensions** are insufficient for reconstruction, highlighting the **limits of PCA in preserving spatial details** at extreme reductions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68729f57-52da-48eb-8bbf-73676070ae57",
   "metadata": {},
   "source": [
    "## 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ae77c-2b9f-492f-a277-593cbefc3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Select a random subset of 40K samples from train set\n",
    "subset_indices = np.random.choice(X.shape[0], 40000, replace=False)\n",
    "X_train = X[subset_indices]\n",
    "y_train = y[subset_indices]\n",
    "\n",
    "# Step 4: Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)  # Apply same transformation to test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80943a1-6f91-4128-906a-c1814da74713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Images Shape:\", X_train.shape)  # (40000, 784)\n",
    "print(\"Train Labels Shape:\", y_train.shape)  # (40000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bf1de-88c3-4176-aeb3-6cbc46993a6e",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580618e-b38e-486a-9db8-44bbe9b3af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step 5: Train MLP Classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=50, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predict on test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')  # 'macro' averages across all classes\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"time took :{(end-start):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a2b2e-8c3b-4849-a75f-014ba5f41126",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in [500, 300, 150, 30]:\n",
    "    X_reduced_train, eigenvectors, X_mean_train,_ = pca(X_train, dim)\n",
    "    # X_reduced_test ,eigenvectors, X_mean_test,_ = pca(X_test,dim)\n",
    "    X_centered_test = X_test - X_mean_train  # Use train mean\n",
    "    X_reduced_test = np.dot(X_centered_test, eigenvectors) \n",
    "    \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=50, random_state=42)\n",
    "    mlp.fit(X_reduced_train, y_train)\n",
    "\n",
    "    y_pred = mlp.predict(X_reduced_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')  # 'macro' averages across all classes\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"Dimensions:{dim}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3914b-18f0-4b93-9195-31d5b0951187",
   "metadata": {},
   "source": [
    "# **Observations on MLP Classifier Performance Before and After PCA**\n",
    "\n",
    "## **1. Baseline Model Performance (Before PCA)**\n",
    "- The model trained on the original dataset (without dimensionality reduction) achieved:\n",
    "  - **Accuracy:** 97.73%\n",
    "  - **Precision:** 97.72%\n",
    "  - **Recall:** 97.71%\n",
    "- This shows that the MLP classifier performs **very well** on the raw MNIST dataset with all 784 features.\n",
    "\n",
    "## **2. Impact of Dimensionality Reduction on Performance**\n",
    "### **(a) PCA with 500 Dimensions**\n",
    "- **Accuracy:** 96.90%  \n",
    "- **Precision:** 96.88%  \n",
    "- **Recall:** 96.85%  \n",
    "- The model performance drops slightly, indicating that PCA **removes some minor details** but still preserves most of the relevant information.\n",
    "\n",
    "### **(b) PCA with 300 Dimensions**\n",
    "- **Accuracy:** 97.28%  \n",
    "- **Precision:** 97.25%  \n",
    "- **Recall:** 97.24%  \n",
    "- Performance improves compared to **500 dimensions**, showing that PCA helps **remove redundant features** while keeping important details.\n",
    "\n",
    "### **(c) PCA with 150 Dimensions**\n",
    "- **Accuracy:** 97.34%  \n",
    "- **Precision:** 97.31%  \n",
    "- **Recall:** 97.31%  \n",
    "- Similar to **300 dimensions**, with a **slight improvement** in performance, suggesting that PCA effectively **captures essential features** with fewer dimensions.\n",
    "\n",
    "### **(d) PCA with 30 Dimensions**\n",
    "- **Accuracy:** 97.36%  \n",
    "- **Precision:** 97.37%  \n",
    "- **Recall:** 97.33%  \n",
    "- Surprisingly, the model performs **better than PCA with 500 dimensions**, meaning that **only 30 principal components are enough** to encode the majority of the variance in the MNIST dataset.\n",
    "\n",
    "## **3. Key Insights**   \n",
    "1. **PCA helps remove redundant information:**  \n",
    "   - The **slight improvement in accuracy** for 150 and 30 dimensions suggests that removing unnecessary features **reduces noise** and enhances generalization.\n",
    "\n",
    "2. **Too many dimensions do not necessarily improve results:**  \n",
    "   - The **500-dimension model performed slightly worse** than models trained with 150 and 30 dimensions, likely due to retaining some **irrelevant or redundant** features.\n",
    "\n",
    "## **4. Conclusion**\n",
    "- **PCA is effective for dimensionality reduction** in MNIST without compromising accuracy.\n",
    "- **30 dimensions are sufficient** for high classification performance.\n",
    "- **Higher dimensions (e.g., 500) retain unnecessary details** that slightly reduce generalization.\n",
    "- PCA can **improve computational efficiency** without major losses in classification quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d8b74-e19a-4b34-b454-2c371505a25f",
   "metadata": {},
   "source": [
    "# 4.3 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451bc42-11e8-4d66-96f6-9ab23d19528f",
   "metadata": {},
   "source": [
    "# **How PCA Helps Mitigate the Curse of Dimensionality**\n",
    "\n",
    "## **What is the Curse of Dimensionality?**\n",
    "The **curse of dimensionality** refers to the challenges that arise when working with high-dimensional data, such as:\n",
    "- Increased computational complexity.\n",
    "- Overfitting due to too many features.\n",
    "- Data sparsity, making it harder to find meaningful patterns.\n",
    "\n",
    "## **How PCA Helps Mitigate These Issues**\n",
    "### 1. **Reduces Redundant Features**\n",
    "- In high-dimensional spaces, many features may be correlated and not contribute much new information.\n",
    "- PCA projects data into a lower-dimensional space, **removing redundancy** and keeping only the most important features.\n",
    "\n",
    "### 2. **Improves Model Generalization**\n",
    "- High-dimensional models often **overfit**, capturing noise instead of meaningful patterns.\n",
    "- PCA helps models **focus on the most significant features**, improving performance on unseen data.\n",
    "\n",
    "### 3. **Enhances Computational Efficiency**\n",
    "- Training models in high dimensions requires **more memory and computation**.\n",
    "- PCA reduces dimensions, making learning algorithms **run faster** and with lower memory usage.\n",
    "\n",
    "### 4. **Improves Data Visualization & Interpretation**\n",
    "- High-dimensional data is hard to visualize.\n",
    "- PCA projects data onto **2D or 3D spaces**, making it easier to analyze clusters and patterns.\n",
    "\n",
    "---\n",
    "\n",
    "# **When PCA Might Not Be Effective in High-Dimensional Spaces**\n",
    "While PCA is useful, it **may not always work well**, especially in certain cases:\n",
    "\n",
    "###  **1. Non-Linear Data Distributions**\n",
    "- PCA **assumes linear relationships** between features.\n",
    "- If data has complex **non-linear structures** (e.g., curved manifolds), PCA fails to capture essential information.\n",
    "- 🔹 **Alternative:** Use **t-SNE, UMAP, or Kernel PCA** for non-linear transformations.\n",
    "\n",
    "###  **2. When Variance Does Not Correlate with Importance**\n",
    "- PCA selects components based on **variance**, assuming that high variance features are more important.\n",
    "- In some cases, **low-variance features may be critical** (e.g., medical diagnosis where small variations indicate disease presence).\n",
    "- 🔹 **Alternative:** Use **Lasso regression** or **mutual information-based feature selection**.\n",
    "\n",
    "###  **3. Sparse Data Issues**\n",
    "- If the dataset has many **zero or near-zero values** (e.g., text data in NLP with sparse embeddings), PCA may not work well.\n",
    "- 🔹 **Alternative:** Use **Autoencoders** or **factorization techniques (e.g., NMF)**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**PCA is highly effective for mitigating the curse of dimensionality**, particularly when linear relationships exist and variance is a useful indicator of importance.  \n",
    "**However, it may not work well in non-linear, sparse, or highly structured data**, where other dimensionality reduction methods might be better suited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76c21b-cacf-4e4e-9a25-d8578119fbb5",
   "metadata": {},
   "source": [
    "# **Does PCA Always Capture the Most Informative Directions?**\n",
    "\n",
    "## **PCA Assumption**\n",
    "PCA assumes that **the directions of maximum variance contain the most important information** about the data.  \n",
    "This works well in many cases, but **this assumption is not always valid**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **When PCA's Assumption Might Fail**\n",
    "PCA may fail when:\n",
    "1. **Low-Variance Features are Important**  \n",
    "2. **Non-Linear Relationships Exist**  \n",
    "3. **Class Labels Are Not Captured by Variance**  \n",
    "\n",
    "### **Example: Handwritten Digit Classification (MNIST)**\n",
    "Consider **MNIST**, a dataset of handwritten digits (0-9).  \n",
    "\n",
    "🔹 PCA will find the **principal components** based on pixel intensity variance.  \n",
    "🔹 However, **digit identity (class information) is not always aligned with variance**.  \n",
    "🔹 Some **low-variance features** (e.g., stroke thickness, subtle curve differences) may be critical for distinguishing similar digits (like 3 vs. 8).  \n",
    "\n",
    "**PCA may discard these features, reducing classification performance.**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example: XOR Problem (Non-Linear Data)**\n",
    "The **XOR dataset** is a classic example where PCA fails.  \n",
    "\n",
    "#### **XOR Dataset Structure**\n",
    "- Two classes are **non-linearly separable** (they form an \"XOR\" pattern in feature space).\n",
    "- Variance alone **does not separate the classes**.\n",
    "\n",
    "#### **Why PCA Fails?**\n",
    "- PCA finds directions **maximizing variance**, but **this does not separate the XOR classes**.\n",
    "- PCA projects XOR data in a way that **still makes it non-separable**.  \n",
    "- A **non-linear method** (e.g., **Kernel PCA or t-SNE**) is needed instead.\n",
    "\n",
    "---\n",
    "\n",
    " **PCA is powerful, but its assumption can fail when:**  \n",
    " Important information is in **low-variance directions** (e.g., digit classification).  \n",
    " The data has **non-linear relationships** (e.g., XOR problem).  \n",
    "\n",
    " **Alternative methods like Kernel PCA, t-SNE, or deep learning may be better in these cases.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19e8bc-9878-495c-aaf4-e0ec92de1c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
