{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba640c90-1726-42f2-9765-31e029ca2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de84c1-f451-4f90-9372-b113cad083d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 2\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70762510-8bf8-4f5e-82cf-1257912df87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST(\"./data\", train=True, download=True, transform=transform), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST(\"./data\", train=False, download=True, transform=transform), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d483ff-7cbd-48be-bc22-6aca6170a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Variational Autoencoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 400)\n",
    "        self.fc_mu = nn.Linear(400, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(400, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, 400)\n",
    "        self.fc3 = nn.Linear(400, 28*28)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc2(z))\n",
    "        return torch.sigmoid(self.fc3(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 28*28))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e490f1-ab57-4b23-a0e3-f4a279328a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar,recon,KL):\n",
    "    loss = 0\n",
    "    if recon == \"BCE\":\n",
    "        loss = F.binary_cross_entropy(recon_x, x.view(-1, 28*28), reduction='sum')\n",
    "    elif recon == \"MSE\":\n",
    "        loss = F.mse_loss(recon_x, x.view(-1, 28*28), reduction='sum')\n",
    "    KL_divergence = 0\n",
    "    if KL:\n",
    "        KL_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return loss + KL_divergence\n",
    "\n",
    "# Training loop\n",
    "def train(recon = \"BCE\",KL = True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        # print(\"shape of data\",data.shape)\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar,recon,KL)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "\n",
    "# Testing function\n",
    "def test(recon = \"BCE\",KL = True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_mu = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar,recon,KL).item()\n",
    "            all_mu.append(mu.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "    return test_loss / len(test_loader.dataset), np.vstack(all_mu), np.hstack(all_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57ff14-78ad-4487-9ef8-698e368d3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "\n",
    "# Loss function with BCE and KL\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and visualize latent space\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train()\n",
    "    test_loss, latent_means, labels = test()\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot latent space visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(latent_means[:, 0], latent_means[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Latent Dimension 1\")\n",
    "plt.ylabel(\"Latent Dimension 2\")\n",
    "plt.title(\"Latent Space Visualization of Means With BCE and KL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21decf7-7056-4f3f-ad82-29e84f0edc9b",
   "metadata": {},
   "source": [
    "# **Observations on Latent Space Visualization of VAE (BCE + KL)**\n",
    "\n",
    "## **1. Well-Structured Latent Space**\n",
    "- The latent space has a meaningful clustering of digits, with different digit classes (color-coded) forming distinct but overlapping regions.\n",
    "- This indicates that the VAE has learned a structured latent representation, where similar digits are placed closer together.\n",
    "\n",
    "## **2. Overlapping and Separation of Classes**\n",
    "- Some digit classes (e.g., 1 and 9) are more separated, while others (e.g., 3, 5, and 8) overlap significantly.\n",
    "- This overlap suggests that the VAE finds certain digits more similar in feature space, possibly due to shared stroke patterns.\n",
    "\n",
    "## **3. Gaussian-Like Distribution**\n",
    "- The latent space appears roughly Gaussian, centered around (0,0) with spread-out clusters. This is a good sign since the **KL divergence loss forces the latent space to follow a normal distribution**.\n",
    "- Some points extend outward, which might indicate that certain digits have more variation in their representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691ec34-af2f-4c2d-960d-2b5305280bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from a 2D Gaussian Grid and visualize reconstructions\n",
    "grid_x = np.linspace(-3, 3, 10)\n",
    "grid_y = np.linspace(-3, 3, 10)\n",
    "\n",
    "grid_z = torch.tensor(np.array([[x, y] for x in grid_x for y in grid_y]), dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructions = model.decode(grid_z).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(reconstructions[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Reconstructed Images from 2D Gaussian Grid Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829b7f1-ad41-423e-8ba9-0c4af1710399",
   "metadata": {},
   "source": [
    "## Observations from the avove grid\n",
    "- The images are not great but close enough to the images in the MNIST dataset\n",
    "- There are few images which mixec 4 and 9 , 3 and 8 since they are kind of similar and the overlap in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36289cff-b254-4365-8686-54a8af58010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function with no BCE but with KL\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and visualize latent space\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(\"\")\n",
    "    test_loss, latent_means, labels = test(\"\")\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot latent space visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(latent_means[:, 0], latent_means[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Latent Dimension 1\")\n",
    "plt.ylabel(\"Latent Dimension 2\")\n",
    "plt.title(\"Latent Space Visualization of Means Only with KL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a27cb7-d467-4631-a7ba-ecd6a4521c98",
   "metadata": {},
   "source": [
    "## **KL Divergence Loss**\n",
    "### **Purpose:**\n",
    "- Regularizes the latent space by forcing it to resemble a standard normal distribution (\\(\\mathcal{N}(0, I)\\)).\n",
    "- Ensures continuity and smoothness, allowing for meaningful interpolation between latent points.\n",
    "\n",
    "### **Effects:**\n",
    "- **High KL loss** → The model's latent space is too constrained, leading to blurry outputs.\n",
    "- **Low KL loss** → The model learns useful latent representations.\n",
    "- If KL loss dominates, the latent space collapses to a single point (**posterior collapse**).\n",
    "\n",
    "### **Observations from KL-Only Results:**\n",
    "- **Latent Space:** Completely collapsed, with almost no variation.\n",
    "- **Loss:** Loss immediately drops to near-zero, indicating the model is ignoring the latent space and ignores the mistakes or variation from the original data.\n",
    "- **Issue:** Without BCE, the model does not reconstruct meaningful outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9394ba-81d1-420a-8e5e-6aeeb02963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from a 2D Gaussian Grid and visualize reconstructions\n",
    "grid_x = np.linspace(-3, 3, 10)\n",
    "grid_y = np.linspace(-3, 3, 10)\n",
    "\n",
    "grid_z = torch.tensor(np.array([[x, y] for x in grid_x for y in grid_y]), dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructions = model.decode(grid_z).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(reconstructions[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Reconstructed Images from 2D Gaussian Grid Samples; when NO BCE but with KL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ea32a-b9d4-4274-b629-0fc36ba0e3cd",
   "metadata": {},
   "source": [
    "## Observations from the above grid\n",
    "- The reconstructed images appear to be random noise instead of meaningful structures. This suggests that the decoder has not learned to generate structured outputs..\n",
    "- Without BCE  the model lacks the incentive to reconstruct meaningful images, leading to poor outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ab1ce-4ba1-4d9c-8ab6-e4e4759d1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function with  BCE but not with KL\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and visualize latent space\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(KL = False)\n",
    "    test_loss, latent_means, labels = test(KL = False)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot latent space visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(latent_means[:, 0], latent_means[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Latent Dimension 1\")\n",
    "plt.ylabel(\"Latent Dimension 2\")\n",
    "plt.title(\"Latent Space Visualization of  Only with BCE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc760c-a2ca-467f-9674-abf733a9aa74",
   "metadata": {},
   "source": [
    "## **Reconstruction Loss (BCE/MSE)**\n",
    "### **Purpose:**\n",
    "- Ensures that the reconstructed output is as close as possible to the original input.\n",
    "- Encourages the decoder to learn meaningful features from the latent space.\n",
    "\n",
    "### **Effects:**\n",
    "- **High BCE loss** → Poor reconstruction quality.\n",
    "- **Low BCE loss** → The model accurately reconstructs inputs.\n",
    "- If only BCE is used (without KL), the model behaves like a regular Autoencoder, and the latent space may lack smoothness and structure.\n",
    "\n",
    "### **Observations from BCE-Only Results:**\n",
    "- **Latent Space:** Without KL, clusters form, but they are scattered and not regularized.\n",
    "- **Loss Curve:** The loss decreases steadily, showing improvement in reconstruction.\n",
    "- **Issue:** Overfitting can occur, as the latent space is not constrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9dca27-ce97-4da2-8d3c-fcff9addffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from a 2D Gaussian Grid and visualize reconstructions\n",
    "grid_x = np.linspace(-3, 3, 10)\n",
    "grid_y = np.linspace(-3, 3, 10)\n",
    "\n",
    "grid_z = torch.tensor(np.array([[x, y] for x in grid_x for y in grid_y]), dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructions = model.decode(grid_z).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(reconstructions[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Reconstructed Images from 2D Gaussian Grid Samples ; when BCE but not with KL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b71777-a9c9-4cf5-a886-31e640ffa5af",
   "metadata": {},
   "source": [
    "## Observations from the avove grid\n",
    "- The images above are better than just KL loss,the reconstructions now resemble actual digits.\n",
    "- Since KL divergence is missing, the latent space does not follow a structured distribution\n",
    "- This can cause the sampled images to transition less smoothly or produce out-of-distribution artifacts.\n",
    "- Some transitions between digits are not smooth, and certain regions in the latent space might not generate meaningful samples.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f82ba1-f7b8-4397-bb43-393822743c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function with  MSE and KL\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train using recon MSE and visualize latent space \n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(\"MSE\")\n",
    "    test_loss, latent_means, labels = test(\"MSE\")\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot latent space visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(latent_means[:, 0], latent_means[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Latent Dimension 1\")\n",
    "plt.ylabel(\"Latent Dimension 2\")\n",
    "plt.title(\"Latent Space Visualization of Means With MSE and KL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92de203-774c-4976-8ccb-7ab215e79d85",
   "metadata": {},
   "source": [
    "## **Latent Space Structure**\n",
    "- The latent space shows **clustered but overlapping digit distributions**.\n",
    "- This means the **KL divergence successfully regularized the latent space**, ensuring a smooth transition between digits.\n",
    "- However, some classes are **not entirely separated**, which might cause mixed reconstructions when sampling from certain regions.\n",
    "- The overall structure seems to be **more continuous**, making **interpolation more reliable** than the BCE-only case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ffc6a-7ee7-4783-b174-8d843eae753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from a 2D Gaussian Grid and visualize reconstructions\n",
    "grid_x = np.linspace(-3, 3, 10)\n",
    "grid_y = np.linspace(-3, 3, 10)\n",
    "\n",
    "grid_z = torch.tensor(np.array([[x, y] for x in grid_x for y in grid_y]), dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructions = model.decode(grid_z).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(reconstructions[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Reconstructed Images from 2D Gaussian Grid Samples when ; MSE and KL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11c5fa-c99e-474d-ac3a-484886be5311",
   "metadata": {},
   "source": [
    "## **Reconstruction Quality**\n",
    "- The reconstructions are fairly good images\n",
    "- This is expected since **MSE loss tends to produce smoother images** compared to BCE loss, which focuses more on pixel-wise classification.\n",
    "- The transitions between digits are smoother than the **BCE-only case**, showing **better latent space regularization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a03c9-0dc4-4e66-a50f-25acea08ea3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
